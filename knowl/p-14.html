<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<p>Finally, the development of Checkit itself raises several interesting RUME questions that Clontz aims to explore in future collaborations with education researchers. For example, the process of authoring an exercise that aims to assess a particular learning outcome is much simpler than authoring an exercise template to be seeded with randomized data. In mathematics this is sometimes achievable by simply randomizing numerical elements of the exercise; for example, the template <code class="code-inline tex2jax_ignore">{{a}}x + {{b}}y = {{c}}</code> expressing the standard equation of a line might be randomized to <span class="process-math">\(4x+5y=-2\text{,}\)</span> <span class="process-math">\(-3x+y=0\text{,}\)</span> and so on. However, what constraints are appropriate for this randomization to ensure it still serves as a valid assessment of a given outcome? Certainly, it seems unlikely that examples such as <span class="process-math">\(531284127x-4512874312y=341893123\)</span> are necessary. But should the line occassionally be expressed in point-slope form <span class="process-math">\(y=-mx+b\)</span> instead? And how can the stem of a question be randomized to ensure that students are synthesizing complete instructions, rather than only memorizing patterns developed from seeing solutions to similarly-generated exercises?</p>
<span class="incontext"><a href="background.html#p-14" class="internal">in-context</a></span>
</body>
</html>
